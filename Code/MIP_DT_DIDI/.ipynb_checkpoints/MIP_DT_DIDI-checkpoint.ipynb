{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames;\n",
    "using JuMP\n",
    "using Gurobi\n",
    "using CSV\n",
    "using JLD\n",
    "using Combinatorics\n",
    "\n",
    "\n",
    "\n",
    "using Printf\n",
    "# In this code we save the prediction value as a new column called \"class_pred\" in the data_train and data_test;\n",
    "# So if we have already a column with this name we may get in trouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Arguments that we need from the user\n",
    "#####################################################\n",
    "data_group = \"compas\"; # or compas, adult, german\n",
    "# kamiran_version = 1 if we use the kamiran version of the data; \n",
    "# In this case SP is just the difference between positive outcome probability between non_deprived and deprived group\n",
    "kamiran_version = 0; \n",
    "sample = 2;\n",
    "depth = 1;\n",
    "lambda = 0;\n",
    "time_limit = 100;\n",
    "\n",
    "\n",
    "\n",
    "train_file_name = data_group*\"_train_\"*string(sample)*\".csv\";\n",
    "test_file_name = data_group*\"_test_\"*string(sample)*\".csv\";\n",
    "calibration_file_name = data_group*\"_calibration_\"*string(sample)*\".csv\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Reading data\n",
    "#####################################################\n",
    "if kamiran_version == 1\n",
    "    file_path = \"./../../DataSets/KamiranVersion/\";\n",
    "else\n",
    "    file_path = \"./../../DataSets/\";\n",
    "end\n",
    "\n",
    "data_train = CSV.read(file_path*train_file_name ,DataFrame);\n",
    "N_train = size(data_train,1);\n",
    "\n",
    "data_test = CSV.read(file_path*test_file_name ,DataFrame);\n",
    "N_test = size(data_test,1);\n",
    "\n",
    "data_calibration = CSV.read(file_path*calibration_file_name ,DataFrame);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_group == \"compas\"\n",
    "    # Need to specify name of the column containing the class label and protected feature\n",
    "    class = :target; # name of the class label in the dataset\n",
    "    B = :race; # protected feautre\n",
    "\n",
    "    # Need to specify which column are categorical and which columns are non-categorical (quantitative)\n",
    "    # categorical features\n",
    "    F_c = [:sex,:c_charge_degree];\n",
    "    nf_c=size(F_c,1);\n",
    "\n",
    "    # quantitative features\n",
    "    F_q=[:age_cat,:priors_count,:length_of_stay];\n",
    "    nf_q=size(F_q,1);\n",
    "    \n",
    "    # We need the class label to be binary (0,1). In this data, the class lables are (1,2)\n",
    "    data_train[!,class] .-= 1;\n",
    "    data_test[!,class] .-= 1;\n",
    "    \n",
    "    \n",
    "    deprvied_group = 1\n",
    "    positive_class = 0\n",
    "end\n",
    "\n",
    "if data_group == \"german_binary\"\n",
    "    # Need to specify name of the column containing the class label and protected feature\n",
    "    class = :target; # name of the class label in the dataset\n",
    "    B = :age; # protected feautre\n",
    "\n",
    "    # Need to specify which column are categorical and which columns are non-categorical (quantitative)\n",
    "    # categorical features\n",
    "    F_c = [:check_acc, :credit_history, :purpose, :saving_amo, :present_employment, :p_status, :guatan, :property, :installment, :Housing, :job, :telephn, :foreign_worker];\n",
    "    nf_c=size(F_c,1);\n",
    "\n",
    "    # quantitative features\n",
    "    F_q=[:month_duration, :Credit_amo, :instalrate, :present_resident, :existing_cards, :no_people];\n",
    "    nf_q=size(F_q,1);\n",
    "    \n",
    "    # We need the class label to be binary (0,1). In this data, the class lables are (1,2)\n",
    "    data_train[!,class] .-= 1;\n",
    "    data_test[!,class] .-= 1;\n",
    "    \n",
    "    positive_class = 1\n",
    "    deprvied_group = 1\n",
    "end\n",
    "\n",
    "\n",
    "if data_group == \"adult\"\n",
    "    # Need to specify name of the column containing the class label and protected feature\n",
    "    class = :target; # name of the class label in the dataset\n",
    "    B = :sex; # protected feautre\n",
    "\n",
    "    # Need to specify which column are categorical and which columns are non-categorical (quantitative)\n",
    "    # categorical features\n",
    "    F_c = [:workclass, :education, :marital_status, :occupation, :relationship, :race, :native_country];\n",
    "    nf_c=size(F_c,1);\n",
    "\n",
    "    # quantitative features\n",
    "    F_q=[:hours_per_week, :age_group, :fnlwgt, :capital];\n",
    "    nf_q=size(F_q,1);\n",
    "    \n",
    "    # We need the class label to be binary (0,1). In this data, the class lables are (1,2)\n",
    "    data_train[!,class] .-= 1;\n",
    "    data_test[!,class] .-= 1;\n",
    "    \n",
    "    deprvied_group = 1\n",
    "    positive_class = 1\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels(data_train[!,B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4629"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7716262975778547"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp = 4\n",
    "size(data_train[(data_train[!,:priors_count].<=2) .& (data_train[!,B].==xp),:],1)/\n",
    "size(data_train[(data_train[!,B].==xp),:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ind (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################\n",
    "#index function\n",
    "#####################################################\n",
    "function ind(x)\n",
    "    if x==true\n",
    "  1\n",
    "    else\n",
    "  0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_tree (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################\n",
    "#Tree structures\n",
    "#####################################################\n",
    "function get_tree(d)\n",
    "    nn = 2^d-1\n",
    "    nl = 2^d\n",
    "    \n",
    "    left=Vector{Array{Int64}}();\n",
    "    right = Vector{Array{Int64}}();\n",
    "    for n in 1:nn\n",
    "        crnt_depth = Int(floor(log(2,n)))\n",
    "        number_nodes_crnt_depth = 2^crnt_depth\n",
    "        num_leafs_under_n = 2^(d - crnt_depth)\n",
    "        \n",
    "        first_leaf_idx = (n-number_nodes_crnt_depth)*num_leafs_under_n+1\n",
    "        last_leaf_idx = first_leaf_idx+ num_leafs_under_n -1\n",
    "        mid_leaf_idx = Int(floor((first_leaf_idx+last_leaf_idx)/2))\n",
    "        n_lef = [first_leaf_idx:1:mid_leaf_idx;]\n",
    "        n_right = [mid_leaf_idx+1:1:last_leaf_idx;]\n",
    "        push!(left, n_lef);\n",
    "        push!(right, n_right);\n",
    "    end\n",
    "    \n",
    "    nn, nl, left, right\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_MIP_model (generic function with 1 method)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_MIP_model(nn, nl, left, right, data_train, B, class, F_c, F_q, lambda)\n",
    "    # In this model we assume the class labels in data_train are binary (0 and 1)\n",
    "    data = deepcopy(data_train);\n",
    "    N = size(data,1)\n",
    "\n",
    "    # Parameters\n",
    "    M=200;\n",
    "    M2=200;\n",
    "    ep=0.1;\n",
    "\n",
    "    fair = 1; # whether we have fairness penalty (fair=1) or not (fair=0)\n",
    "    if lambda ==0\n",
    "        fair = 0\n",
    "    end\n",
    "\n",
    "    ############################################\n",
    "    # Defining the decision variables\n",
    "    ############################################\n",
    "    model = Model()\n",
    "\n",
    "    # r[i,l] = |y_i - z_l|*x[i,l] which is the prediction error if i is assigned to l\n",
    "    # sum(r[i,l] for l=1:nl)=|y_i - yhat_i|\n",
    "    @variable(model, r[1:N,1:nl]>=0); \n",
    "\n",
    "    # x[i,l] denotes if datapoint i is assigned to leaf l\n",
    "    @variable(model, x[1:N,1:nl],Bin);\n",
    "\n",
    "    # ac[n,j]=1 means that we split on categorical feature j at node n\n",
    "    # in the paper we call this variable p[n,j]\n",
    "    @variable(model, ac[1:nn,F_c],Bin); \n",
    "\n",
    "    # aq[n,j]=1 means that we split on quantitative feature j at node n\n",
    "    @variable(model, aq[1:nn,F_q],Bin); \n",
    "\n",
    "    # b[n] is the cut-off value at node n if we split on quantitative features\n",
    "    @variable(model, b[1:nn]); \n",
    "\n",
    "    @variable(model, gp[1:N,1:nn]>=0);\n",
    "    @variable(model, gn[1:N,1:nn]>=0);\n",
    "\n",
    "    # if wq[i,n]=1 datapoint i should go left at node n; Also this means that we have splitted on a quantitative feature\n",
    "    @variable(model, wq[1:N,1:nn],Bin);\n",
    "\n",
    "    # if wc[i,n]=1 datapoint i should go left at node n; Also this means that we have splitted on a categorical feature\n",
    "    @variable(model, wc[1:N,1:nn],Bin);\n",
    "\n",
    "\n",
    "    @variable(model, s[1:nn,f in F_c, k in levels(data[!,f])],Bin);\n",
    "\n",
    "\n",
    "    # v[i,l] = |y_i- z_l|\n",
    "    @variable(model, v[1:N,1:nl]>=0); \n",
    "\n",
    "    #z_l is the binary prediction that we make at leaf node l\n",
    "    @variable(model, z[1:nl], Bin); \n",
    "\n",
    "\n",
    "    if fair == 1\n",
    "        # to linearize the prediction in the penalty function\n",
    "        # rp[i,l] = x[i,l]*z[l]\n",
    "        # sum(rp[i,l] for l=1:nl) = yhat_i\n",
    "        @variable(model, rp[1:N,1:nl]>=0); \n",
    "\n",
    "        #to linearize the absolute value in the penalty function\n",
    "        # rpp[y,xp] = |P(y) - P(y|xp)|\n",
    "        @variable(model, rpp[y in levels(data[!,class]), xp in levels(data[!,B])] >=0);\n",
    "    end;\n",
    "\n",
    "    ############################################\n",
    "    # objective\n",
    "    ############################################\n",
    "    if fair == 1\n",
    "        # 1/N*sum(|y_i - yhat_i| over i) + lambda* sum(|P(y) - P(y|xp)| over y and xp)\n",
    "        @objective(model, Min , (1-lambda)*(1/N)*sum(r[i,l] for i = 1:N , l=1:nl) +\n",
    "            lambda*(sum(rpp[y,xp] for y in levels(data[!,class]), xp in levels(data[!,B])) ) );\n",
    "    else\n",
    "        # 1/N*sum(|y_i - yhat_i| over i)\n",
    "        @objective(model, Min , (1/N)*sum(r[i,l] for i = 1:N , l=1:nl));\n",
    "    end;\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    #Fairness constraints\n",
    "    ############################################\n",
    "    if fair == 1\n",
    "        # rpp[y,xp] = |P(y) - P(y|xp)|\n",
    "        for y in levels(data[!,class]), xp in levels(data[!,B])\n",
    "            # Let's |i: data[i,B]=xp|    \n",
    "            N_xp = size(data[(data[!,B] .== xp),:],1)\n",
    "            if (N_xp!= 0)\n",
    "            @constraint(model, sum(ind(sum(rp[i,l] for l=1:nl)==y) for i=1:N)/ N\n",
    "                             - sum(ind(sum(rp[i,l] for l=1:nl)==y) for i=1:N if (data[i,B]==xp) )/N_xp<= rpp[y,xp]);\n",
    "            @constraint(model, -sum(ind(sum(rp[i,l] for l=1:nl)==y) for i=1:N)/ N\n",
    "                               +sum(ind(sum(rp[i,l] for l=1:nl)==y) for i=1:N if (data[i,B]==xp) )/N_xp<= rpp[y,xp]);\n",
    "\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # rp[i,l] = x[i,l]*z[l] so sum(rp[i,l] over l) = yhat_i\n",
    "        for i in 1:N, l in 1:nl\n",
    "            @constraint(model,rp[i,l]<=M2*x[i,l]);\n",
    "            @constraint(model,rp[i,l]<=z[l]);\n",
    "            @constraint(model,rp[i,l]>=z[l]-M2*(1-x[i,l]));\n",
    "        end\n",
    "    end;\n",
    "\n",
    "    ##############################################\n",
    "    # nodes constraints\n",
    "    ##############################################\n",
    "    for n in 1:nn\n",
    "        @constraint(model,sum(ac[n,f] for f in F_c)+sum(aq[n,f] for f in F_q)==1);\n",
    "    end\n",
    "\n",
    "\n",
    "    ##############################################\n",
    "    # categoricals\n",
    "    ##############################################\n",
    "    for n in 1:nn, f in F_c, k in levels(data[!,f])\n",
    "        @constraint(model,s[n,f,k] <= ac[n,f]);\n",
    "    end\n",
    "\n",
    "    for i in 1:N, n in 1:nn\n",
    "        @constraint(model,wc[i,n] == sum(sum(s[n,f,k]*ind(data[i,f]==k) for k in levels(data[!,f])) for f in F_c) );\n",
    "        for l in left[n]\n",
    "            @constraint(model,x[i,l] <= wc[i,n]+1-sum(ac[n,f] for f in F_c));\n",
    "        end\n",
    "        for l in right[n]\n",
    "            @constraint(model,x[i,l]<=1-wc[i,n]+1-sum(ac[n,f] for f in F_c));\n",
    "        end\n",
    "    end\n",
    "    ##############################################\n",
    "    # non categoricals\n",
    "    ##############################################\n",
    "    for i in 1:N, n in 1:nn\n",
    "        @constraint(model,b[n]-sum(aq[n,f]*data[i,f] for f in F_q) == gp[i,n]-gn[i,n]);\n",
    "        @constraint(model,gp[i,n]<=M*wq[i,n]);\n",
    "        @constraint(model,gn[i,n]<=M*(1-wq[i,n]));\n",
    "        @constraint(model,gp[i,n]+gn[i,n]>=ep*(1-wq[i,n])); \n",
    "        for l in right[n]\n",
    "            @constraint(model,x[i,l]<=1-wq[i,n]+1-sum(aq[n,f] for f in F_q));\n",
    "        end\n",
    "        for l in left[n]\n",
    "            @constraint(model,x[i,l]<=wq[i,n]+1-sum(aq[n,f] for f in F_q));\n",
    "        end\n",
    "    end\n",
    "    ##############################################\n",
    "    # linearize prediction error\n",
    "    ##############################################\n",
    "    for i in 1:N\n",
    "        @constraint(model,sum(x[i,l] for l=1:nl)==1);\n",
    "        for l in 1:nl\n",
    "            @constraint(model,r[i,l]<=M2*x[i,l]);\n",
    "            @constraint(model,r[i,l]<=v[i,l]);\n",
    "            @constraint(model,r[i,l]>=v[i,l]-M2*(1-x[i,l]));\n",
    "            @constraint(model,v[i,l]>=  data[i,class]-z[l]); \n",
    "            @constraint(model,v[i,l]>= -data[i,class]+z[l]); \n",
    "        end\n",
    "    end\n",
    "                                    \n",
    "    \n",
    "    model\n",
    "                                    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2023-03-03\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 2 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 92585 rows, 46302 columns and 268495 nonzeros\n",
      "Model fingerprint: 0xa9e8d24f\n",
      "Variable types: 27775 continuous, 18527 integer (18527 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-01, 2e+02]\n",
      "  Objective range  [2e-04, 2e-04]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e-01, 2e+02]\n",
      "Found heuristic solution: objective 0.4549579\n",
      "Presolve removed 32403 rows and 13887 columns\n",
      "Presolve time: 0.70s\n",
      "Presolved: 60182 rows, 32415 columns, 208318 nonzeros\n",
      "Variable types: 9259 continuous, 23156 integer (23156 binary)\n",
      "\n",
      "Deterministic concurrent LP optimizer: primal and dual simplex\n",
      "Showing first log only...\n",
      "\n",
      "Concurrent spin time: 0.16s\n",
      "\n",
      "Solved with dual simplex\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 9622 iterations, 2.63 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0 9264    0.45496    0.00000   100%     -    6s\n",
      "     0     0    0.00000    0 9265    0.45496    0.00000   100%     -   13s\n",
      "     0     0    0.00000    0 9266    0.45496    0.00000   100%     -   19s\n",
      "     0     0    0.00000    0  206    0.45496    0.00000   100%     -   21s\n",
      "     0     0    0.00000    0  650    0.45496    0.00000   100%     -   21s\n",
      "     0     0    0.00000    0  650    0.45496    0.00000   100%     -   21s\n",
      "     0     0    0.00000    0  410    0.45496    0.00000   100%     -   21s\n",
      "     0     0    0.00000    0  211    0.45496    0.00000   100%     -   21s\n",
      "H    0     2                       0.4005185    0.00000   100%     -   21s\n",
      "     0     2    0.00000    0  211    0.40052    0.00000   100%     -   21s\n",
      "H    4     6                       0.3493195    0.00000   100%   933   21s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 4\n",
      "  Cover: 86\n",
      "  Implied bound: 265\n",
      "  Flow cover: 134\n",
      "  Zero half: 42\n",
      "\n",
      "Explored 556 nodes (56746 simplex iterations) in 22.38 seconds\n",
      "Thread count was 4 (of 4 available processors)\n",
      "\n",
      "Solution count 3: 0.34932 0.400518 0.454958 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.493195074530e-01, best bound 3.493195074530e-01, gap 0.0000%\n",
      "\n",
      "User-callback calls 6434, time in user-callback 0.01 sec\n"
     ]
    }
   ],
   "source": [
    "# Let's build the model\n",
    "nn, nl, left, right = get_tree(depth);\n",
    "DT_model = get_MIP_model(nn, nl, left, right, data_train, B, class, F_c, F_q, lambda);\n",
    "\n",
    "# Let's solve the model\n",
    "set_optimizer_attribute(DT_model, \"TimeLimit\", time_limit);\n",
    "set_optimizer(DT_model, Gurobi.Optimizer);\n",
    "JuMP.optimize!(DT_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3493195074530022"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution_summary(DT_model)\n",
    "objective_value(DT_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "print_tree (generic function with 1 method)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_predictions(nn,nl,left,right, ac, aq, b, s, z, time_limit, data_org, F_c, F_q)\n",
    "    data = deepcopy(data_org);\n",
    "    N = size(data,1)\n",
    "    \n",
    "    #####################################################################\n",
    "    model_pred = Model(Gurobi.Optimizer)\n",
    "    set_optimizer_attribute(model_pred, \"TimeLimit\", time_limit)\n",
    "    M=200;\n",
    "    M2=200;\n",
    "    ep=0.1;\n",
    "    #####################################################################\n",
    "    @variable(model_pred, x[1:N,1:nl],Bin);\n",
    "    @variable(model_pred, gp[1:N,1:nn]>=0);\n",
    "    @variable(model_pred, gn[1:N,1:nn]>=0);\n",
    "    @variable(model_pred, wq[1:N,1:nn],Bin);\n",
    "    @variable(model_pred, wc[1:N,1:nn],Bin);\n",
    "    \n",
    "    # routing constraints for categorical features\n",
    "    for i in 1:N,n in 1:nn\n",
    "        @constraint(model_pred,wc[i,n] == sum(sum(s[n,f,k]*ind(data[i,f]==k) for k in levels(data[!,f])) for f in F_c) );\n",
    "        for l in left[n]\n",
    "            @constraint(model_pred,x[i,l] <= wc[i,n]+1-sum(ac[n,f] for f in F_c));\n",
    "        end\n",
    "        for l in right[n]\n",
    "            @constraint(model_pred,x[i,l]<=1-wc[i,n]+1-sum(ac[n,f] for f in F_c));\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # routing constraints for non categoricals features\n",
    "    for i in 1:N, n in 1:nn\n",
    "        @constraint(model_pred,b[n]-sum(aq[n,f]*data[i,f] for f in F_q) == gp[i,n]-gn[i,n]);\n",
    "        @constraint(model_pred,gp[i,n]<=M*wq[i,n]);\n",
    "        @constraint(model_pred,gn[i,n]<=M*(1-wq[i,n]));\n",
    "        @constraint(model_pred,gp[i,n]+gn[i,n]>=ep*(1-wq[i,n])); #if b=criteria then w is 1 and go left\n",
    "        for l in right[n]\n",
    "            @constraint(model_pred,x[i,l]<=1-wq[i,n]+1-sum(aq[n,f] for f in F_q));\n",
    "        end\n",
    "        for l in left[n]\n",
    "            @constraint(model_pred,x[i,l]<=wq[i,n]+1-sum(aq[n,f] for f in F_q));\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for i in 1:N\n",
    "        @constraint(model_pred,sum(x[i,l] for l=1:nl)==1);\n",
    "    end\n",
    "    #####################################################################\n",
    "    set_silent(model_pred)\n",
    "    optimize!(model_pred)\n",
    "    x = round.(JuMP.value.(x));\n",
    "    pred= x*z;\n",
    "    acc = sum(pred .== data[!,class])/N\n",
    "    \n",
    "    \n",
    "    pred, acc\n",
    "end\n",
    "\n",
    "\n",
    "function get_SP(data, label_souce, B, deprvied_group, positive_class, kamiran_version)\n",
    "    #data = deepcopy(data_org);\n",
    "    disc = 0\n",
    "    if kamiran_version==1\n",
    "        sp_non_deprived = size(data[(data[!,B] .!= deprvied_group) .& (data[!,label_souce] .== positive_class) ,:],1)/\n",
    "        size(data[(data[!,B] .!= deprvied_group) ,:],1)\n",
    "        \n",
    "        sp_deprived = size(data[(data[!,B] .== deprvied_group) .& (data[!,label_souce] .== positive_class) ,:],1)/\n",
    "        size(data[(data[!,B] .== deprvied_group) ,:],1)\n",
    "        \n",
    "        disc = sp_non_deprived - sp_deprived\n",
    "    else\n",
    "        \n",
    "        for (p, p_prime) in combinations(levels(data[!,B]),2)\n",
    "            N_p = size(data[(data[!,B] .== p) ,:],1)\n",
    "            N_p_prime = size(data[(data[!,B] .== p_prime) ,:],1)\n",
    "            \n",
    "            if N_p!=0 & N_p_prime!=0\n",
    "                sp_p = size(data[(data[!,B] .== p) .& (data[!,label_souce] .== positive_class) ,:],1)/ N_p\n",
    "\n",
    "                sp_p_prime = size(data[(data[!,B] .== p_prime) .& (data[!,label_souce] .== positive_class) ,:],1)/N_p_prime\n",
    "\n",
    "                disc = max(disc, abs(sp_p-sp_p_prime))\n",
    "            end\n",
    "            \n",
    "        end\n",
    "    end\n",
    "        \n",
    "        \n",
    "    disc\n",
    "end\n",
    "\n",
    "\n",
    "function get_DIDI(data, class, label_souce, B)\n",
    "    #data = deepcopy(data_org);\n",
    "    disc = 0\n",
    "    N = size(data,1)\n",
    "    for y in levels(data[!,class]), xp in levels(data[!,B]) \n",
    "        N_xp = size(data[(data[!,B] .== xp),:],1)\n",
    "        if (N_xp!= 0)\n",
    "            P_y = size(data[(data[!,label_souce] .== y) ,:],1)/N\n",
    "            P_y_given_xp = size(data[(data[!,B] .== xp) .& (data[!,label_souce] .== y) ,:],1)/N_xp\n",
    "            disc+= abs(P_y-P_y_given_xp)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    disc\n",
    "end\n",
    "\n",
    "function print_tree(nn, nl, data, F_c, F_q, ac, aq, b, s, z)\n",
    "    for n in 1:nn\n",
    "        for j in F_c\n",
    "            if ac[n,j]==1\n",
    "                left_levels = []\n",
    "                for k in levels(data[!,j])\n",
    "                    if s[n,j,k]==1\n",
    "                        append!(left_levels,k)\n",
    "                    end\n",
    "                end\n",
    "                @printf(\"######## Node %d: Go to left if %s is in %s \\n\", n,j,string(left_levels))\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        for j in F_q\n",
    "            if aq[n,j]==1\n",
    "                @printf(\"######## Node %d: Go to left if %s <= %f \\n\", n,j,b[n])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    for n in 1:nl\n",
    "        @printf(\"######## Leaf %d: %d \\n\",n ,z[n])\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values of each decision variable\n",
    "# We also round the values of binary decision variable to make sure we have sharp 0 and 1s\n",
    "ac = round.(JuMP.value.(DT_model[:ac]));\n",
    "aq = round.(JuMP.value.(DT_model[:aq]));\n",
    "b = JuMP.value.(DT_model[:b]);\n",
    "s = round.(JuMP.value.(DT_model[:s]));\n",
    "z = round.(JuMP.value.(DT_model[:z]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Node 1: Go to left if priors_count <= 2.900000 \n",
      "######## Leaf 1: 0 \n",
      "######## Leaf 2: 1 \n"
     ]
    }
   ],
   "source": [
    "print_tree(nn, nl, data_train, F_c, F_q, ac, aq, b, s, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2023-03-03\n",
      "train_acc=\t0.6506804925469863\n",
      "Academic license - for non-commercial use only - expires 2023-03-03\n",
      "test_acc=\t0.6493843162670123\n",
      "Academic license - for non-commercial use only - expires 2023-03-03\n",
      "calibration_acc=\t0.13156189241736876\n"
     ]
    }
   ],
   "source": [
    "train_pred,train_acc = get_predictions(nn,nl,left,right, ac, aq, b, s, z, time_limit, data_train, F_c, F_q);\n",
    "data_train[!,:class_pred] = train_pred;\n",
    "println(\"train_acc=\\t\",train_acc)\n",
    "\n",
    "\n",
    "test_pred,test_acc = get_predictions(nn,nl,left,right, ac, aq, b, s, z, time_limit, data_test, F_c, F_q);\n",
    "data_test[!,:class_pred] = test_pred;\n",
    "println(\"test_acc=\\t\",test_acc)\n",
    "\n",
    "\n",
    "calibration_pred,calibration_acc = get_predictions(nn,nl,left,right, ac, aq, b, s, z, time_limit, data_calibration, F_c, F_q);\n",
    "data_calibration[!,:class_pred] = calibration_pred;\n",
    "println(\"calibration_acc=\\t\",calibration_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_didi=\t0.6351325494276616\n",
      "train_pred_didi=\t0.9004162181585729\n",
      "train_data_sp=\t0.14113723746501716\n",
      "train_pred_sp=\t0.18707380684599317\n"
     ]
    }
   ],
   "source": [
    "train_data_didi = get_DIDI(data_train, class, class, B);\n",
    "train_pred_didi = get_DIDI(data_train, class, :class_pred, B);\n",
    "\n",
    "train_data_sp = get_SP(data_train, class,       B, deprvied_group, positive_class, kamiran_version);\n",
    "train_pred_sp = get_SP(data_train, :class_pred, B, deprvied_group, positive_class, kamiran_version);\n",
    "\n",
    "println(\"train_data_didi=\\t\",train_data_didi)\n",
    "println(\"train_pred_didi=\\t\",train_pred_didi)\n",
    "println(\"train_data_sp=\\t\",train_data_sp)\n",
    "println(\"train_pred_sp=\\t\",train_pred_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_didi=\t0.6174482569699942\n",
      "test_pred_didi=\t0.9319089462847763\n",
      "test_data_sp=\t0.13698584064843777\n",
      "test_pred_sp=\t0.1903238825547372\n"
     ]
    }
   ],
   "source": [
    "test_data_didi = get_DIDI(data_test, class, class, B);\n",
    "test_pred_didi = get_DIDI(data_test, class, :class_pred, B);\n",
    "\n",
    "test_data_sp = get_SP(data_test, class,       B, deprvied_group, positive_class, kamiran_version);\n",
    "test_pred_sp = get_SP(data_test, :class_pred, B, deprvied_group, positive_class, kamiran_version);\n",
    "\n",
    "println(\"test_data_didi=\\t\",test_data_didi)\n",
    "println(\"test_pred_didi=\\t\",test_pred_didi)\n",
    "println(\"test_data_sp=\\t\",test_data_sp)\n",
    "println(\"test_pred_sp=\\t\",test_pred_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibration_data_didi=\t0.6413296826496888\n",
      "calibration_pred_didi=\t0.4493646698641933\n",
      "calibration_data_sp=\t0.0\n",
      "calibration_pred_sp=\t0.18694409415720892\n"
     ]
    }
   ],
   "source": [
    "calibration_data_didi = get_DIDI(data_calibration, class, class, B);\n",
    "calibration_pred_didi = get_DIDI(data_calibration, class, :class_pred, B);\n",
    "\n",
    "calibration_data_sp = get_SP(data_calibration, class,       B, deprvied_group, positive_class, kamiran_version);\n",
    "calibration_pred_sp = get_SP(data_calibration, :class_pred, B, deprvied_group, positive_class, kamiran_version);\n",
    "\n",
    "println(\"calibration_data_didi=\\t\",calibration_data_didi)\n",
    "println(\"calibration_pred_didi=\\t\",calibration_pred_didi)\n",
    "println(\"calibration_data_sp=\\t\",calibration_data_sp)\n",
    "println(\"calibration_pred_sp=\\t\",calibration_pred_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"./../../Results/MIP_DIDI_compas_train_1.csv_depth_1_lambda_0.csv\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################################\n",
    "#Saving results\n",
    "####################################################################################\n",
    "filePath = \"./../../Results/\"*\"MIP_DIDI_\"*train_file_name*\"_depth_\"* string(depth)*\"_lambda_\"*string(lambda)*\".csv\";\n",
    "\n",
    "header=[\"Approach\", \"train_file_name\", \"test_file_name\", \"kamiran_version\", \"N_train\", \"depth\", \"lambda\", \"time_limit\", \n",
    "           \"obj_val\", \"gap\", \n",
    "           \"train_acc\", \"train_data_didi\", \"train_pred_didi\", \"train_data_sp\", \"train_pred_sp\",\n",
    "           \"test_acc\", \"test_data_didi\", \"test_pred_didi\", \"test_data_sp\", \"test_pred_sp\",\n",
    "           \"calibration_acc\", \"calibration_data_didi\", \"calibration_pred_didi\", \"calibration_data_sp\", \"calibration_pred_sp\"];\n",
    "\n",
    "row_input=[\"MIP_DIDI\", train_file_name, test_file_name, kamiran_version, N_train, depth, lambda, time_limit, \n",
    "           objective_value(DT_model), relative_gap(DT_model::Model), \n",
    "           train_acc, train_data_didi, train_pred_didi, train_data_sp, train_pred_sp,\n",
    "           test_acc, test_data_didi, test_pred_didi, test_data_sp, test_pred_sp,\n",
    "           calibration_acc, calibration_data_didi, calibration_pred_didi, calibration_data_sp, calibration_pred_sp];\n",
    "\n",
    "CSV.write(filePath,  Tables.table(row_input), header = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
